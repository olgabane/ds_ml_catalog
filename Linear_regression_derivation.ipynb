{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I work through two derivations for solving linear regession:\n",
    "1. Ordinary least squares. \n",
    "2. Maximum likelihood.\n",
    "\n",
    "    - univariate linear regeression. \n",
    "    - multivariate linear regression.\n",
    "\n",
    "**NOTE**: LaTeX doesn't render properly in this notebook on github - see PDF instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares derivation of linear regression\n",
    "\n",
    "$Loss function = \\sum\\limits_{i=1}^n(y_{i}-\\hat{y_{i}})^2$\n",
    "\n",
    "   \n",
    "$\\text{Define loss function and expand:} $ \n",
    "\n",
    "$Q = \\sum\\limits_{i=1}^n(y_{i}-\\hat{y_{i}})^2,   \\hat{y_{i}} = mx_{i} + b $  \n",
    "\n",
    "$Q(m, b) = \\sum\\limits_{i=1}^n(y_{i}- (mx_{i} + b))^2$  \n",
    "\n",
    "$Q(m, b) = \\sum\\limits_{i=1}^n(y_{i}- mx_{i} - b)^2$  \n",
    "\n",
    "$Q(m, b) = \\sum\\limits_{i=1}^n(y_{i}^2 - y_{i}mx_{i} - y_{i}b - y_{i}mx_{i} + m^2x_{i}^2 + mx_{i}b - y_{i}b + mx_{i}b + b^2)$\n",
    "\n",
    "$Q(m, b) = \\sum\\limits_{i=1}^ny_{i}^2 - \\sum\\limits_{i=1}^ny_{i}mx_{i} - \\sum\\limits_{i=1}^ny_{i}b - \\sum\\limits_{i=1}^ny_{i}mx_{i} + \\sum\\limits_{i=1}^nm^2x_{i}^2 + \\sum\\limits_{i=1}^nmx_{i}b - \\sum\\limits_{i=1}^ny_{i}b + \\sum\\limits_{i=1}^nmx_{i}b + \\sum\\limits_{i=1}^nb^2$\n",
    "\n",
    "$Q(m, b) = \\sum\\limits_{i=1}^ny_{i}^2 + \\sum\\limits_{i=1}^nm^2x_{i}^2 + \\sum\\limits_{i=1}^nb^2 - \\sum\\limits_{i=1}^n2y_{i}mx_{i} - \\sum\\limits_{i=1}^n2y_{i}b + \\sum\\limits_{i=1}^n2mx_{i}b $\n",
    "\n",
    "\n",
    "Differentiate with respect to **m** and **b**:\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial m} = 2m\\sum{x_{i}^2} - 2\\sum\\limits_{i=1}^ny_{i}x_{i} + 2b\\sum\\limits_{i=1}^nx_{i}$\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial b} = 2bN - 2\\sum{y_{i}} + 2m\\sum{x_{i}}, \\textit{N = number of points}$\n",
    "\n",
    "\n",
    "We want to minimize m and b, so set both to 0:\n",
    "\n",
    "$2m\\sum{x_{i}^2} - 2\\sum{y_{i}x_{i}} + 2b\\sum{x_{i}} = 0$\n",
    "\n",
    "$2bN - 2\\sum{y_{i}} + 2m\\sum{x_{i}} = 0$\n",
    "\n",
    "\n",
    "Solve for b first:  \n",
    "$b = \\dfrac{2\\sum{y_{i}} - 2m\\sum{x_{i}}}{2N} $  \n",
    "\n",
    "$b = \\dfrac{\\sum{y_{i}} - m\\sum{x_{i}}}{N} $\n",
    "\n",
    "Substitute b and solve for m:\n",
    "\n",
    "\n",
    "$2m\\sum{x_{i}^2} - 2\\sum{y_{i}x_{i}} + 2\\dfrac{\\sum{y_{i}} - m\\sum{x_{i}}}{N}\\sum{x_{i}} = 0$ \n",
    "\n",
    "$m\\sum{x_{i}^2} - \\sum{y_{i}x_{i}} + \\dfrac{\\sum{y_{i}} - m\\sum{x_{i}}}{N}\\sum{x_{i}} = 0$\n",
    "\n",
    "$m\\sum{x_{i}^2} - \\sum{y_{i}x_{i}} + \\dfrac{\\sum{y_{i}}\\sum{x_{i}} - m(\\sum{x_{i}})^2}{N} = 0$  \n",
    "\n",
    "$mN\\sum{x_{i}^2} - N\\sum{y_{i}x_{i}} + \\sum{y_{i}}\\sum{x_{i}} - m(\\sum{x_{i}})^2 = 0$ \n",
    "\n",
    "$mN\\sum{x_{i}^2} - m(\\sum{x_{i}})^2 = N\\sum{y_{i}x_{i}} - \\sum{y_{i}}\\sum{x_{i}}$  \n",
    "\n",
    "$m(N\\sum{x_{i}^2} - (\\sum{x_{i}})^2) = N\\sum{y_{i}x_{i}} - \\sum{y_{i}}\\sum{x_{i}}$  \n",
    "\n",
    "$m = \\dfrac{N\\sum{y_{i}x_{i}} - \\sum{y_{i}}\\sum{x_{i}}}{N\\sum{x_{i}^2} - (\\sum{x_{i}})^2}$  \n",
    "\n",
    "This solution assumes:\n",
    "\n",
    "- Residuals are normally distributed. \n",
    "- Residuals have an equal variance (no heteroscedasticity).\n",
    "- The means of the residuals is 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least squares derivation of linear regression (abridged)\n",
    "\n",
    "1\\. Define loss function and expand:    \n",
    "$Q(m, b) = \\sum\\limits_{i=1}^n(y_{i}-\\hat{y_{i}})^2 = \\sum\\limits_{i=1}^n(y_{i}- (mx_{i} + b))^2$  \n",
    "...  \n",
    "$Q(m, b) = \\sum\\limits_{i=1}^ny_{i}^2 + \\sum\\limits_{i=1}^nm^2x_{i}^2 + \\sum\\limits_{i=1}^nb^2 - \\sum\\limits_{i=1}^n2y_{i}mx_{i} - \\sum\\limits_{i=1}^n2y_{i}b + \\sum\\limits_{i=1}^n2mx_{i}b $\n",
    "\n",
    "2\\. Differentiate with respect to **m** and **b** and set to 0 because we want to minimize error:\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial m} = 0 = 2m\\sum{x_{i}^2} - 2\\sum\\limits_{i=1}^ny_{i}x_{i} + 2b\\sum\\limits_{i=1}^nx_{i}$\n",
    "\n",
    "$\\frac{\\partial Q}{\\partial b} = 0 = 2bN - 2\\sum{y_{i}} + 2m\\sum{x_{i}}$  \n",
    "\n",
    "$\\textit{     N = number of points}$\n",
    "\n",
    "3\\. Solve for **m** and **b**: \n",
    "\n",
    "$m = \\dfrac{N\\sum{y_{i}x_{i}} - \\sum{y_{i}}\\sum{x_{i}}}{N\\sum{x_{i}^2} - (\\sum{x_{i}})^2}$   \n",
    "$b = \\dfrac{\\sum{y_{i}} - m\\sum{x_{i}}}{N} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood derivation of linear regression\n",
    "\n",
    "**Model to be predicted:**   \n",
    "$y = mx + b$\n",
    "\n",
    "1. PDF for normally-distributed variable: \n",
    "$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$  \n",
    "$ PDF(X) = \\frac{1}{\\sqrt {2\\pi\\sigma^2} }e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}$  \n",
    "2. Treat the linear equation that we need to find as the mean of a normal distribution.  \n",
    "$ P(y|x; m, b, \\sigma^2) = \\frac{1}{\\sqrt {2\\pi\\sigma^2} }e^{\\frac{-(y-(mx+b))^2}{2\\sigma^2}} $\n",
    "\n",
    "    _**Why**_? When we draw a line through some points, the distance between the line and each point is a residual. We make three assumptions about the residuals:\n",
    "    - Residuals are normally distributed. \n",
    "    - Residuals have an equal variance (no heteroscedasticity).\n",
    "    - The means of the residuals is 0. \n",
    "    \n",
    "    \n",
    "3. Likelihood function for all observed points (x, y) is the product of the probability density for each point:  \n",
    "$ L(m, b, \\sigma^2) = \\frac{1}{\\sqrt {2\\pi\\sigma^2} }\\prod\\limits_{i=1}^ne^{\\frac{-(y_i-(mx_i+b))^2}{2\\sigma^2}} $  \n",
    "4. Goal: find parameters **b**, **m** and **$\\sigma$** that **maximize _L_.**  \n",
    "  \n",
    "  \n",
    "5. Convert to log likelihood for easier math, **maximize _log(L)_**:  \n",
    "$ log(L) = log[\\frac{1}{\\sqrt {2\\pi\\sigma^2} }\\prod\\limits_{i=1}^ne^{\\frac{-(y_i-(mx_i+b))^2}{2\\sigma^2}}]$   \n",
    "...  \n",
    "$ log(L) = -\\frac{1}{2}log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(y_i-(mx_i+b))^2$ \n",
    "\n",
    "6. Or, **minimize** negative log likelihood:   \n",
    "$ -log(L) = \\frac{1}{2}log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2}\\sum\\limits_{i=1}^n(y_i-(mx_i+b))^2$\n",
    "\n",
    "7. Let's _imagine_ that our variance term is a fixed constant.     \n",
    "$-log(L) = \\sum\\limits_{i=1}^n(y_{i}- (mx_{i} + b))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood derivation for multi-variate linear regression \n",
    "\n",
    "**Model to be predicted**:   \n",
    "$y = \\theta_0 + \\theta_1 x_1 + \\theta_1 x_2 + \\theta_1 x_3 + ... \\theta_n x_n$  \n",
    "$Y = X_{n\\space x\\space d} * \\theta_{d\\space x\\space 1}$\n",
    "\n",
    "$ y \\sim \\mathcal{N}(X\\theta, \\sigma^2)$\n",
    "\n",
    "Given \\#1 and \\#2 above: \n",
    "1. Likelihood function for all observed points (x, y) is the product of the probability density for each point:  \n",
    "$ L(\\theta X, \\sigma^2) = \\frac{1}{\\sqrt {2\\pi\\sigma^2} }\\prod\\limits_{i=1}^ne^{\\frac{-(y_i-(x_i\\theta))^2}{2\\sigma^2}} $\n",
    "\n",
    "    - Note technically $\\frac{1}{\\sqrt {2\\pi\\sigma^2} }$ can be to the n here, but this won't make a difference in the end. \n",
    "\n",
    "2. Re-write with a sigma, then convert to matrix notation:   \n",
    "$ L(\\theta X, \\sigma^2) = \\frac{1}{\\sqrt {2\\pi\\sigma^2} }e^{\\frac{\\sum\\limits_{i=1}^n-(y_i-(x_i\\theta))^2}{2\\sigma^2}} $  \n",
    " \n",
    "    $ L(\\theta X, \\sigma^2) = \\frac{1}{\\sqrt {2\\pi\\sigma^2} }e^-{\\frac{(Y - X\\theta)^T(Y-X\\theta)}{2\\sigma^2}} $\n",
    "    \n",
    "     $ L(\\theta X, \\sigma^2) = \\frac{1}{\\sqrt {2\\pi\\sigma^2} }e^-{\\frac{|Y - X\\theta|^2}{2\\sigma^2}} $\n",
    "    \n",
    "3. Maximizing likelihood is equivalent to minimizing sum of errors*. Partially differentiate with respect to theta and set to 0.\n",
    "\n",
    "    $|Y - X\\theta|^2 = Y^2 - 2X^TY\\theta + X^TX\\theta^2$  \n",
    "    $\\frac{\\partial |Y - X\\theta|^2}{\\partial \\theta} = -2X^TY + 2X^TX\\theta$  \n",
    "    $0 = -2X^TY + 2X^TX\\theta = -X^TY + X^TX\\theta$  \n",
    "    $\\theta = \\frac{X^TY}{X^TX} = (X^TY)(X^TX)^-1$\n",
    "\n",
    "\\* https://ocw.mit.edu/courses/mathematics/18-443-statistics-for-applications-fall-2006/lecture-notes/section15.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
